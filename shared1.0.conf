hive{
  tables{
    account = ${hive.dataBase}".account"
    accountMetricsView = ${hive.dataBase}".account_metrics_view"
    accountRegressions = ${hive.dataBase}".account_regressions"
    accountRegressionsInc = ${hive.dataBase}".account_regressions_inc"
    accountRegressionsView = ${hive.dataBase}".account_regressions_view"
    accountRegressionsMetrics = ${hive.dataBase}".account_regressions_metrics"
    externalIndicatorsTable = ${hive.dataBase}".externalIndicators"
    recommendationsTable = ${hive.dataBase}".recommendation"
    recommendationsLatest = ${hive.dataBase}".recommendation_latest"
    indicatorsPerRegressionTable = ${hive.dataBase}".indicatorsperregressions"
    indicatorspereventsTable = ${hive.dataBase}".indicatorsperevents"
    indicatorsperscenariosTable = ${hive.dataBase}".indicatorsperscenarios"
    expectedRecommendations = "cliff.expectedRecommendations"
    expectedRecommendationsEventIndex = "cliff.expectedRecommendationsEventIndex"
    expectedRegressionsIndicators = "cliff.expectedRegressionsIndicators"
    expectedRegressionsMetrics = "cliff.expectedregressionsmetrics"
    expectedEventsIndicators = "cliff.expectedEventsIndicators"
    expectedScenariosIndicators = "cliff.expectedScenariosIndicators"
    expectedRedundantRegressions = "cliff.expectedRedundantRegressions"
    expectedRedundantEvents = "cliff.expectedRedundantEvents"
    expectedRedundantScenarios = "cliff.expectedRedundantScenarios"
    recommendationForBDDIndicators = "cliff.recommendationForBDDIndicators"
    indicatorsForIntegrationEventIndex = "cliff.indicatorsForIntegrationEventIndex"
    indicatorsForBDDEventIndex = "cliff.indicatorsForBDDEventIndex"
    expectedexaustive = "cliff.expectedexaustive"
    expectedOperations = "cliff.expectedOperations"
    runSummary = ${hive.dataBase}".run_summary"
    feedBackRegressionTestsTable = ${hive.dataBase}".feedBackRegressionTests"
    feedBackRegressionHitsTable = ${hive.dataBase}".feedBackRegressionHits"
    originalRecommendationsTable = ${hive.dataBase}".originalRecommendations"
    rocCurveTable = ${hive.dataBase}".ROC_curve"
    dimPatternsStagingTable = ${hive.dataBase}".dim_patterns_staging"
    flavoringRegressionStatusTable = ${hive.dataBase}".flavoring_regression_status_inc"
    flavoringRegressionStatusView = ${hive.dataBase}".flavoring_regression_status_view"
    patternsWithoutTemplate = "cliff.patterns_without_template"
  }
  queries{
    useCliff = "use "${hive.dataBase}
    createDataBase = "create database if not exists "${hive.dataBase}
    account="SELECT * FROM "${hive.tables.account}
    regression="SELECT * FROM "${hive.tables.accountRegressionsView}" where account_id = "
  }
  dataBase = "cliff"
  dataBase = ${?dataBase}
}

sql{
  tables{
    test = "dbo.jdbc_test"
    feedbackSummary = "dbo.feedback_summary"
    feedbackSummaryTest = "dbo.feedback_summary_test"
  }
  exec{
    test = "testExec"
    bugAnalysis = "bucketAnalysisExec"
  }
  dataBase = "CLIFF"
  jdbc{
    url = "jdbc:sqlserver://papdb-test.intel.com:3180"
    driver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
  }
  user = "epmsysadm"
  password = "jst4spexe"
}

Data_Loader{
  format = "csv" // choose from: csv, parquet
  numOfPartitions = -1
  partitionFactor = 2
  cluster_id_col = "cluster_id"
  templateType = "TEMPLATE"
  configurationTypes = "ICON:KNOB_MC:PARAMETER:MESSAGE:SCRIPT:XHCITrTRBX:USB3PDSX:knobs:FEKNOBS:KNOB:KNOBS:knob_CSI:test_name:run_mode:USB_temp:FE_LISA:test_case:mem_conf:CmdOption:proj_config:CoMix_cfg:RAND_ENV:bias_file:FETESTKNOBS:uCode_cfg:fuse_cfg:seq_info:TEST_name:SEQ_setting:"${Data_Loader.templateType}
  configurationTypes = ${?configuration.types}
  operationsSymbol = "SEQ:SEQUENCE:CoMix_Scenario"
  operationsSymbol = ${?operation.types}
  configurationsMinimalAppearancesInRegression = 2 //1 to leave all configurations
  configurationsFilterTH = 100
  customParserCSV = false
  eventsToIgnore = "" // separate events by :
  fixTypes = true
  filterTestsByBusinessLogic = true
  knobs_metadata{
    toFilterKnobs = false
    knobsSpecificDate = 0
    knobsThreshold = 10 // in percents
    knobsUpToDate = 3
  }
  conradications_data{
    conradicationsDate = 0
  }
  big_core_clusters = "ooo_glc,mlc_glc,mlc_pm_glc,fe_glc,msid_glc"
}

Data_Loader_SQL{
  runBucketAnalysis = true
  defaultDaysForRegressionArriveTime = 2
}

hdfs{
  account{
    path="/appdata/cliff/fhsa/cliff"
  }
  metadata{
    path = "/appdata/cliff/fhsa/metadata/cliff"
  }
  item{
    path = "/appdata/cliff/fhsa/item"
  }
  knobsMetadata{
    path = ${hdfs.item.path}
  }
  commands{
    ls="hadoop fs -ls "
    mkdir="hadoop fs -mkdir "
    mv="hadoop fs -mv "
  }
  metadata_dir ="metadata"
  weekly_metadata_dir ="weekly_metadata"
  activeRegressionPath= "/active"
  history= "historyMergedFiles"
  mergedFolder = "/appdata/cliff/fhsa/cliff/mergedFiles/"
  command = "hadoop fs -ls "
}

RandomForests{
  // for BDD
  randomSeed = 111// set seed for all random. Affects also the "ExRfFuse" model
  pctIconsInIteration = 1
  removeMainFeatureAfterPct =1

  // for normal running
  //  randomSeed = -1 // do random
  //  pctIconsInIteration = 0.8
  //  removeMainFeatureAfterPct =0.6
  numOfTrees = 20 //Relevant also to the "ExRfFuse" model
  maxDepth = 15  //Relevant also to the "ExRfFuse" model
  maxFeaturesInResult = 3
  maxIterations = 3
  //  importanceThreshold =0.05

  minimalRareEventHits = 12
  minimalRareEventHits = ${?minimal.rare.event.hits}
  maximalRareEventHits = 46
  maximalRareEventHits = ${?maximal.rare.event.hits}
  maxNumberOfEvents = 0 //upper bound for running events even if maximalRareEventHits is set to high number. use Zero(0) to ignore and to take only minimal and maximal RareEventHits into accout
}

EventsPriorityIndex{
  enabled = true
  topIndex = 2
  maxDaysToTake = 21
  maxRegressionsToTake = 2
}

Evaluator {
  useCoverageModel = true
  minSeedsToRunPerRecommendation = 100
  recallThreshold = 0.15
  numRecommendations = 3
  beta = 5.0
  scoreMethod = 0 // 0 for fScoresRatio (legacy score), 1 for hitRatesRatio X recallIprovement (new score)
  precisionFilteringMultiplier = 0.0 //Set to 0.0 to disable precision filtering. The precision threshold is precisionFilteringMultiplier/minSeedsToRunPerRecommendation (while 1/minSeedsToRunPerRecommendation is the minimal hitRate)
  maximalFAR = 100.0 //The maximal false accept rate for calculating the optimal threshold using ROC. Values in the range [0.0,100.0]. 100.0 will return all recommendations without filtering
  confidenceInterval = 0.95
}

PreProcess{
  similarityThreshold = 0.9
  maxOperationsBetweenEvents = 0
  unpersistAllDataAfterPivot = false
}

PostProcess{
  typeToAddToRecommendationsIfMissing = ""
  typeToAddToRecommendationsIfMissing = ${?Data_Loader.templateType}
  templateValueToAddToOOORecommendationsIfMissing = "$MODEL_ROOT/core/ooo/cte/ooo_te/sequence_library/ooo_seq_lib_cliff.e" //the template value to add to each OOO recommendation if template is missing
}

operations{
  maxOperationsBeforeEvent = 100
}

exhaustive{ //all parameters also relevant for the "ExRfFuse" model
  numOfPartitions = 200
  minSupportUpperBound = 0.9
  minSupportLowerBound = 0.15
  supportIterationSize = 0.05
  minSetSize = 1 //minimal recommendation length
  maxSetSize = 4//maximal recommendation length
  minItemSets = 60000//leave it 10000 for exhaustive and between 100000 - 200000 for "ExRfFuse"(for "ExRfFuse" the parameter is the maximal value of recommendations for evaluation - has influence on performance due to amount of evaluations)
  thresholdForCommonConfs = 0.9
  thresholdForCommonOperations = 0.95
  timeoutMs = 30000
  useYarnTimeout = true
  pruneFPGrowth = true
  maxPotentials = 27
  maxThreshold = 0.7
}

crossValidation{
  useCV = false
  totalFolds = 3
  minimalHitsForCV = 36
  evaluateTrainSet = true
  filterTrainSet = true
  takeTopNfromTrainSet = true
  trainSetTopN = 80
}

logManager{
  FlowManagerLogLevel = "info" //options: "debug", "info","error", "fatal","warn","trace"
  FlowManagerExLogLevel = "info"
  ExternalIndicatorsExecutorLogLevel = "info"
  CompareBDDLogLevel = "info"
}

merge_files{
  src = " --src "
  dest= " --dest "
  report = " --report "
  command_prefix = "spark2-submit --class com.intel.aa.merge.MergeWholeFiles --master yarn-cluster --num-executors 10 --executor-cores 10 --executor-memory 10g /appbin/cliff/merge-files/merge-files-assembly.jar --partNum 5"
  part = "part"
  mergedDir = "mergedFiles"
  reportDir = "reportFiles"
  tempDir = "tempRegression"
  minNumberOfFilesToStartMerge = 150

  outputPartsDefault = 20
  outputPartsMin = 1
  outputMinKeywords = "rpt,hsd"
  skipMergeKeywords = "mergedFiles,tempRegression,temp,integration,merged,nagiosHealthCheckDir"
  mergeEncrypted = true
  encryptedSources = "utdb" // comma separated list
  encryptedTextDelimiter = "##"
  decryptedSourceKeywords = "decrypted" // comma separated list
  old_regressions_merger{
    daysPassedThreshold = 180
    enable = false
  }
  stableRegressionMerger{
    minNumberOfFilesToStartMerge = 50
    daysPassedThreshold = 7
  }
  regressionCountValidation{
    skip=true
    skipCountKeywords="turnin"
  }
}

monitor_files{
  maxDaysBeforeIgnore = 7
  maxDaysBeforeIgnoreFeedback = ${monitor_files.maxDaysBeforeIgnore} // same as above but for feedback regs
  flavoringMonitorEnabled = false
  getRegressionNameAndFeedbackFlagFromFiles = false
}

longRunsNotifications{
  contextTH = 0.5
  feedbackTH = 1
  regressionTH = 9
  runtimeTH = 24
}

mailer{
  recipients = "cliff_support@intel.com,oren.david.kimhi@intel.com,karin.elias@intel.com" // use a comma separated list of addresses: "nir.zohar@intel.com, chen.zelig@intel.com, zohar.meir@intel.com"
  errorRecipients = "cliff.dev@intel.com"
  clusters{ // name should be the same as the name in hadoop
    fe_snc  = ${mailer.recipients}
    mlc_snc = "nadav.lewkowicz@intel.com,"${mailer.recipients}
    mlc_pm_snc = "rotem.tabach@intel.com,"${mailer.recipients}
    meu_snc = "elazar.nativ@intel.com,"${mailer.recipients}
    ooo_snc = "ilya.epshtein@intel.com,guy.tal@intel.com,"${mailer.recipients}
    fe_glc  = ${mailer.recipients}
    mlc_glc = "nadav.lewkowicz@intel.com,"${mailer.recipients}
    mlc_pm_glc = "rotem.tabach@intel.com,"${mailer.recipients}
    meu_glc = "elazar.nativ@intel.com,"${mailer.recipients}
    ooo_glc = "ilya.epshtein@intel.com,guy.tal@intel.com,"${mailer.recipients}
    msid_glc = ${mailer.recipients}
    fe_glc_b0  = ${mailer.recipients}
    mlc_glc_b0 = "nadav.lewkowicz@intel.com,"${mailer.recipients}
    mlc_pm_glc_b0 = "rotem.tabach@intel.com,"${mailer.recipients}
    meu_glc_b0 = "elazar.nativ@intel.com,"${mailer.recipients}
    ooo_glc_b0 = "ilya.epshtein@intel.com,guy.tal@intel.com,"${mailer.recipients}
    msid_glc_b0 = ${mailer.recipients}
  }
  sender = "cliff_support@intel.com" // change the "From:" field to something other than sys_cliffdev, to allow replying to the mail
  errorSender = "cliff.dev@intel.com"
  sendInitialSummary = false
  sendFinalSummary = false
  sendFinalSummaryExternal = false
  sendIndicators = true
  sendIndicatorsExternal = true
  sendAAErrors = true
  sendMergerErrors = true
  sendMonitorErrors = true
  summaryStartDate = "2017-12-25"
  summaryFinishDate = "2018-01-01"
  maxListSize = 30 // used to determine maximum amount of elemnts to display in summary mails
  AAErrorsMaxMessageLength = 40000
  MaxMailBodyLength = 40000
  smtp{
    host = "mail.intel.com"
    port = 25
  }
}

autosys{
  failOnMonitorError = false
  failOnMergerError = false
}

environment{
  qa = "hdqa1gw903c"
  preprod = "hdpp1gw903c"
  prod = "fmhdprd1gw903c"
  autosys_qa = "hdqa1as301c"
  autosys_preprod = "hdpp1asgw001c"
  autosys_prod = "fmhdprd1asgw1c"
}
